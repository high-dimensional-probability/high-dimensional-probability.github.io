---
layout:      post
title:       "Exercise 4.6: Too many linear classifiers"
sorting_tag: "406"
tags:        [chapter 4]
comments:    true
---

For any fixed matrix $$X = [ x_1; \ldots; x_n ] \in \R^{n \times d} $$ and
a vector $$\varepsilon = ( \varepsilon_1 , \ldots , \varepsilon_n ) \in \R^d$$,
there exists at least one solution $$\tilde{\theta} \in \R^d$$ to the linear
system
\begin{align}
  X \tilde{\theta} = \varepsilon \, ,
\end{align}
since by assumption $$d \geq n$$. Since $$\varepsilon_i \in \{ -1, 1 \}$$,
$$\tilde{\theta} \neq 0$$, and thus we can take
$$\theta = \tilde{\theta} / \| \tilde{\theta} \|_2 $$, and by
$$\sign(\langle x, \theta \rangle) = \sign(\langle x , \tilde{\theta} \rangle)$$
for all $$x$$, we have
\begin{align}
  \E\_\varepsilon \left[
    \sup\_{f \in \mathscr{F}}
      \left|
        \frac{1}{n}
        \sum\_{i=1}^n
          \varepsilon\_i f(x\_i)
      \right|
  \right]
  =
  \frac{1}{n}
  \E\_\varepsilon \left[
    \sum\_{i=1}^n
      \varepsilon\_i
      \sign(\varepsilon_i)
  \right]
  =
  1
  \, .
\end{align}
By Proposition 4.12, the above implies the class of linear classifiers is too
rich when we are in the high-dimensional regime ($$d \geq n$$), and thus use
of (unpenalised) empirical risk minimisation will lead to overfitting.
