---
layout:      post
title:       "Exercise 2.22"
sorting_tag: "222"
tags:        [chapter 2, Lipschitz Gaussian, incomplete]
comments:    true
---

## (a)

Since $$ F_d $$ is twice differentiable, it is sufficient to show its Hessian
is everywhere positive semidefinite. As the transformation inside each of the
exponents in $$ F_d $$ is linear in $$ \theta $$, it will suffice to show
\begin{align}
  y \mapsto \log \sum\_{i=1}^n e^{y\_i} \, ,
\end{align}
with $$ n = 2^d $$ is convex. The Hessian of this map is
\begin{align}
  H_y
  =
  \frac{1}{\sum\_i e^{y\_i}} \diag (e^y)
  -
  \frac{1}{\left(\sum\_i e^{y\_i}\right)^2}
  e^y (e^y)^\top
  \, ,
\end{align}
where $$ e^{y} \in \R^n $$ is applied elementwise.
Then for any $$ v \in \R^n $$
\begin{align}
  v^\top H\_y v
  =
  \frac{
    \left( \sum\_i v\_i^2 e^{y\_i} \right)
    \left( \sum\_i e^{y\_i} \right)
    -
    \left( \sum\_i v\_i e^{y\_i} \right)^2
  }{
    \left( \sum\_i e^{y\_i} \right)^2
  }
  \, .
\end{align}
The proof is concluded by an application of the Cauchy-Schwarz inequality
\begin{align}
  \biggl( \sum\_i v\_i \sqrt{e^{y\_i}} \sqrt{e^{y\_i}} \biggr)^2
  \leq
  \biggl( \sum\_i v\_i^2 e^{y\_i} \biggr)
  \biggl( \sum\_i e^{y\_i} \biggr)
  \, .
\end{align}


## (b)

By the Taylor's theorem
$$ \| F_d(\theta) - F_d(\theta') \|_2 \leq \| \nabla F_d (\xi) \|_2 \| \theta - \theta'  \|_2 $$
for some $$ \xi \in \R^{ m } $$, $$ m = {d \choose 2} $$.
Denoting
\begin{align}
  f\_{\theta} (x)
  =
  \frac{1}{\sqrt{d}}
  \sum\_{x \in \lbrace -1 , +1 \rbrace^d}
    \theta\_{jk} x\_j x\_k
  \, ,
\end{align}
and $$ Z_\theta =  e^{F_d(\theta)} $$, we can bound
\begin{align}
  \\| \nabla F_d (\xi) \\|\_2^2
  &=
  \frac{1}{Z\_\xi^2}
  \sum\_{j \neq k}
    \biggl(
      \sum\_{x \in \lbrace -1 , +1 \rbrace^d}
        e^{f\_\xi (x)}
        \nabla\_{\xi\_{jk}} f\_\xi (x)
    \biggr)^2
  \newline
  &\overset{\text{(i)}}{\leq}
  \frac{1}{Z\_\xi^2}
  \sum\_{j \neq k}
    \frac{1}{d}
    \biggl(
    \sum\_{x \in \lbrace -1 , +1 \rbrace^d}
        e^{f\_\xi (x)}
    \biggr)^2
  \newline
  &\overset{\text{(ii)}}{=}
  \frac{1}{d}
  {d \choose 2}
  =
  \frac{d - 1}{2}
  \leq
  d
  \, ,
\end{align}
where (i) follows by combining the monotonicity of $$ z \mapsto z^2 $$,
$$ \nabla_{\xi_{jk}} f_\xi (x) = d^{-1/2} x_j x_k $$, and $$ |x_j x_k | = 1 $$
for all $$ x \in \lbrace -1 , +1 \rbrace^d $$, and (ii) is by the definition
of $$ Z_\xi $$.  


## (c)

We start by bounding $$ \E [F_d (\theta)] $$
\begin{align}
  \E [F\_d (\theta)]
  &\overset{\text{(i)}}{\leq}
  \log \sum\_{x \in \lbrace -1, +1 \rbrace^d}
    \E [ e^{f\_\theta (x)} ]
  \newline
  &\overset{\text{(ii)}}{=}
  \log \biggl[ 2^d \exp \biggl\lbrace
    \frac{\beta^2}{2d} {d \choose 2}
  \biggr\rbrace \biggr]
  \newline
  &=
  d \log 2 + \frac{\beta^2 (d - 1)}{4}
  \leq d \log 2 + d \frac{\beta^2}{4}
  \, ,
\end{align}
where (i) is by the Jensen's inequality, and (ii) by the definition of Gaussian
MGF combined with $$ x \in \lbrace -1 , +1 \rbrace^d $$.

Observing $$ \theta_{jk} \overset{\text{iid}}{\sim} \gauss (0 , \beta^2 ) $$ and
$$ \beta \e_{jk} $$ are equal in distribution for
$$ \e_{jk} \overset{\text{iid}}{\sim} \gauss (0, 1) $$, we can view $$ F_d $$ as
a $$ (\beta\sqrt{d}) $$-Lipschitz function of $$ \e $$, and combine theorem 2.26
with the above bound for $$ \E [F_d (\theta)] $$ to obtain
\begin{align}
  \P \left (F\_d (\theta) \geq d \log 2 + d \frac{\beta^2}{4} + d t \right)
  &\leq
  \P \left(F\_d (\theta) \geq \E [F\_d (\theta)] + dt \right)
  \newline
  &\leq
  2 \exp \left\lbrace
    - \frac{d t^2}{2 \beta^2}
  \right\rbrace
  \, .
\end{align}

<span class="accent">
   The above bound is valid but **does not match the required dependence on $$ \beta $$!**
</span>


## Notes

- When viewed as function of $$ y $$ as in (a), $$ F_d $$ is also known as
the [LogSumExp](https://en.wikipedia.org/wiki/LogSumExp) function.

- The convexity of $$ F_d $$ can be combined with the Jensen's inequality
to derive the lower bound
$$ \E [F_d (\theta)] \geq F_d(\E [\theta]) = F_d (0) = d \log 2$$. Taking
the result from (c), this implies
\begin{align}
  d \log 2
  \leq
  \E [F\_d (\theta)]
  \leq
  d \log 2 + d \frac{\beta^2}{4}
  \, ,
\end{align}
showing that the upper bound becomes tight for small $$ \beta $$.
