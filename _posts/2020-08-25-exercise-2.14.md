---
layout:      post
title:       "Exercise 2.14"
sorting_tag: "214"
tags:        [chapter 2, layer cake, symmetrisation, Chernoff bound, incomplete]
comments:    true
date:        2020-08-25
---

## (a)

By the layer cake trick
\begin{align}
  \V (X)
  =
  \int_0^\infty
    \P ((X - \mu)^2 > t)
    \, \sd t
  \leq
  c_1
  \int_0^\infty
    e^{- c_2 t}
    \, \sd t
  =
  \frac{c_1}{c_2}
  \, .
\end{align}


## (b)

Take $$ X $$ to be a Rademacher random variable. Then any $$ m_X \in [-1 , 1] $$
is a median.


## (c)

We will proceed by proving the bound for the case of (i) large
$$ t \geq \alpha \Delta $$, and (ii) small $$ t < \alpha \Delta $$, where
$$ \Delta = | \E [X] - m_X |$$, and
$$ \alpha > 0 $$ is an auxiliary parameter which we will choose later.

__(i) $$ t \geq \alpha \Delta $$:__
Using $$ | X - m_X | \leq \Delta + | X - E [X] | $$
\begin{align}
  \P (|X - m_X| \geq t)
  &=
  \P \left(|X - m_X| \geq \tfrac{t}{\alpha} + (1 - \tfrac{1}{\alpha})t\right)
  \newline
  &\leq
  \P \left( | X - m_X | \geq \Delta + (1 - \tfrac{1}{\alpha}) t \right)
  \newline
  &\leq
  \P \left( | X - \E [X] | \geq (1 - \tfrac{1}{\alpha}) t \right)
  \leq
  c_1 e^{-c_2 (1 - \alpha^{-1})^2 t^2}
  \, .
\end{align}

__(ii) $$ t < \alpha \Delta $$:__ Noticing the bound from (i) grows
as $$ t \downarrow 0 $$, the strategy we take is to inflate said bound
sufficiently so that it holds vacuously for all $$ 0 \leq t < \alpha \Delta $$.
Introducing another auxiliary parameter $$ \beta \geq 1 $$, we therefore require
\begin{align}
  \beta c_1 e^{-c_2 (1 - \alpha^{-1})^2 t^2}
  \>
  \beta c_1 e^{-c_2 (\alpha - 1)^2 \Delta^2}
  \geq
  1
  \geq
  \P (|X - m_X| \geq t)
  \, .
\end{align}
The crucial observation here is that the assumed bounds
\begin{align}
  \P (X \geq \E [X] + t) &\leq c_1 e^{- c_2 t^2 } \, , \newline
  \P (X \leq \E [X] - t) &\leq c_1 e^{- c_2 t^2 } \, ,
\end{align}
imply that for either $$ \E [X] + t $$ or $$ \E [X] - t $$ to be a median
(i.e., satisfy $$ \P (X \geq m_X) \geq \tfrac{1}{2} $$ and
$$ \P (X \leq m_X) \geq \tfrac{1}{2} $$ respectively),
we must have $$ c_1 e^{- c_2 t^2 } \geq \frac{1}{2} $$.
Hence
\begin{align}
  m_X
  \in
  \left[
    \E[X] - \sqrt{\tfrac{1}{c_2}\log(2 c_1)} ,
    \E[X] + \sqrt{\tfrac{1}{c_2}\log(2 c_1)}
  \right]
  \, ,
\end{align}
which provides a bound for $$ \Delta $$.
Substituting into the above expression
\begin{align}
  \beta c_1 e^{- c_2 (\alpha - 1)^2 \Delta^2}
  \geq
  \beta c_1 e^{- (\alpha - 1)^2 \log 2 c_1}
  =
  \beta c_1 \left( \tfrac{1}{2 c_1} \right)^{(\alpha - 1)^2}
  \geq
  1
  \, .
\end{align}
Since we have no control over $$ c_1 $$, we must set
$$ \alpha = 2 $$ ($$ \alpha = 0 $$ is inadmissable because we have divided by
$$ \alpha $$ above). This then also necessitates setting $$ \beta = 2 $$.

Putting (i) and (ii) together
\begin{align}
  \P (|X - m_X| \geq t)
  \leq
  c_3
  e^{- c_4 t^2}
  \, ,
\end{align}
for $$ c_3 = 2 c_1 $$ and $$ c_4 = \tfrac{c_2}{4} $$ which improves upon
each of the required constants by a factor of two.


## (d)

Applying the Chernoff bound, for any $$ \lambda > 0 $$
\begin{align}
  \P ( | X - \E [X] | \geq t )
  =
  \P \bigl( e^{\lambda (X - \E [X])^2} \geq e^{\lambda t^2} \bigr)
  \leq
  e^{- \lambda t^2}
  \E \bigl[e^{\lambda (X - \E [X])^2}\bigr]
  \, .
\end{align}
Since we are trying to establish $$ X - \E [X] $$ behaves like a sub-Gaussian
random variable, and Theorem 2.6(IV) tells us
$$ \E [ e^{\lambda (X - \E[X])^2} ] < \infty $$ for such random variables,
our goal will be to obtain a similar bound here.

Our approach is based on symmetrisation. Introducing $$ Y $$ as an independent
copy of $$ X $$, we can use the convexity of $$ x \mapsto e^{\lambda x^2} $$
and the Jensen's inequality to obtain
\begin{align}
  \E_X \bigl[e^{\lambda (X - \E_Y [Y])^2}\bigr]
  \leq
  \E_{X, Y} \bigl[e^{\lambda (X - Y)^2}\bigr]
  \, .
\end{align}
The right-hand side can be bounded using the layer cake trick
\begin{align}
  \E \bigl[ e^{\lambda (X - Y)^2} \bigr]
  &=
  \int\_0^\infty
    \P \bigl( e^{\lambda (X - Y)^2} \geq u \bigr) \, \sd u
  \newline
  &\overset{\text{(i)}}{\leq}
  \alpha
  +
  \int\_{\alpha}^\infty
    \P \bigl( e^{\lambda (X - Y)^2} \geq u \bigr) \, \sd u
  \newline
  &\overset{\text{(ii)}}{=}
  \alpha
  +
  \lambda
  \int\_{\frac{\log \alpha}{\lambda}}^\infty
    e^{\lambda v}
    \P (|X - Y| \geq \sqrt{v})
    \, \sd v
  \, ,
\end{align}
where (i) introduces a parameter $$ \alpha \geq 1 $$ which we will optimise
later, and (ii) follows by substituting $$ u = e^{\lambda v} $$.
To bound the second term, we use the triangle inequality
$$ | X - Y | \leq | X - m_x | + | Y - m_X | $$ in combination with
$$ X $$ and $$ Y $$ being i.i.d.
\begin{align}
  \P (| X - Y | < t)
  \geq
  \P ( | X - m\_X | < \tfrac{t}{2} )
  \P ( | Y - m\_X | < \tfrac{t}{2} )
  \, .
\end{align}
Taking complements and rearranging
\begin{align}
  \P (| X - Y | \geq t)
  \leq
  2 \P (|X - m\_X| \geq \tfrac{t}{2})
  -
  \P (|X - m\_X| \geq \tfrac{t}{2})^2
  \leq
  2 \P (|X - m\_X| \geq \tfrac{t}{2})
  \, .
\end{align}
This inequality can then be applied to bound the integral
\begin{align}
  \lambda
  \int\_{\frac{\log \alpha}{\lambda}}^\infty
    e^{\lambda v}
    \P (|X - Y| \geq \sqrt{v})
    \, \sd v
  &\leq
  2 \lambda
  \int\_{\frac{\log \alpha}{\lambda}}^\infty
    e^{\lambda v}
    \P (|X - m_x| \geq \tfrac{\sqrt{v}}{2})
    \, \sd v
  \newline
  &\overset{\text{(i)}}{\leq}
  2 \lambda c\_3
  \int\_{\frac{\log \alpha}{\lambda}}^\infty
    e^{(\lambda - \frac{c\_4}{4}) v}
    \, \sd v
  \newline
  &=
  2 \lambda c\_3
  \biggl[
    \frac{e^{(\lambda - \frac{c\_4}{4}) v}}{\lambda - \frac{c\_4}{4}}
  \biggr]\_{\frac{\log \alpha}{\lambda}}^\infty
  \overset{\text{(ii)}}{=}
  \frac{8 \lambda c\_3}{c\_4 - 4\lambda}
  \alpha^{-\left(\frac{c\_4}{4 \lambda} - 1 \right)}
\end{align}
where (i) is by substitution of the assumed concentration bound around the
median, and (ii) holds whenever $$ \lambda < \frac{c_4}{4} $$.
Substituting into our bound for $$ \E [e^{\lambda (X - Y)^2}] $$
\begin{align}
   \E \bigl[e^{\lambda (X - Y)^2}\bigr]
   &\leq
   \alpha \left(
     1
     +
     c
     \alpha^{-p}
   \right)
   \, ,
\end{align}
where we defined $$ c = \frac{8 \lambda c_3}{c_4 - 4\lambda} $$ and
$$ p = \frac{c_4}{4 \lambda} $$.

We are now in position to optimise over $$ \alpha \geq 1 $$.
Setting the derivative to zero, the unconstrained optimum is
$$ \alpha = [c (p - 1)]^{1 / p} = (2 c_3)^{1 / p} $$.
Since the bound $$ P(|X - m_X| \geq t) \leq c_3 e^{-c_4 t^2} $$ is assumed to
hold for all $$ t \geq 0 $$, taking $$ t \downarrow 0 $$ reveals
$$ c_3 \geq 1 $$ needs to be the case; hence $$ \alpha = (2 c_3)^{1 / p} $$
is also the constrained optimum.
Substituting
\begin{align}
   \E \bigl[e^{\lambda (X - Y)^2}\bigr]
   &\leq
   [
     c (p - 1)
   ]^{1/p}
   +
   c
   =
   (2 c\_3)^{\frac{4 \lambda}{c_4}}
   +
   \frac{8 \lambda c_3}{c_4 - 4\lambda}
   \, .
\end{align}

The only remaining task is to optimise over $$ \lambda < \frac{c_4}{4} $$
\begin{align}
  \P (|X - \E [X]| \geq t)
  \leq
  e^{-\lambda t^2}
  \left[
    (2 c\_3)^{\frac{4 \lambda}{c_4}}
    +
    \frac{8 \lambda c_3}{c_4 - 4\lambda}
  \right]
  \, .
\end{align}
Making $$ \lambda $$ too small would yield the vacuous unit bound, while
approaching $$ \frac{c_4}{4} $$ from below makes the second term explode.
We thus somewhat arbitrarily take the midpoint $$ \lambda = \frac{c_4}{8} $$
which results in
\begin{align}
  \P (|X - \E [X]| \geq t)
  \leq
  c\_1
  e^{- c\_2 t^2 }
  \, ,
\end{align}
with $$ c_1 = \sqrt{2 c_3} (1 + \sqrt{2 c_3}) $$ and
$$ c_2 = \frac{c_4}{8} $$.

<span class="accent">
   The above bound is valid but **does not match the required constants!**
</span>

### Alternative proof technique yielding a better $$ c_1 $$ constant:

Let us start with a combination of the Chernoff bound and the inequality
$$ e^{\lambda |x|} \leq e^{\lambda x} + e^{-\lambda x} $$
\begin{align}
  \P (| X - \E [X] | \geq t)
  &\leq
  \inf\_{\lambda > 0}
    \frac{
      \E [e^{\lambda (X - \E[X])}]
      +
      \E [e^{-\lambda (X - \E[X])}]
    }{e^{\lambda t}}
  \, .
\end{align}
As before, we can introduce $$ Y $$ an independent copy of $$ X $$, and use
the Jensen's inequality to bound the MGF
\begin{align}
  \E_X [e^{\lambda (X - \E_Y[Y])}]
  \leq
  \E_{X, Y} [e^{\lambda (X - Y)}]
  \, .
\end{align}
Assuming integrability (which we prove momentarily), we can rewrite
\begin{align}
  \E [e^{\lambda (X - Y)}]
  =
  1
  +
  \sum\_{n = 1}^{\infty}
    \frac{\lambda^n}{n!}
    \E [(X - Y)^n]
  \, .
\end{align}
Observing that $$ X - Y $$ is symmetric, it is clear that every finite _odd_
moment vanishes, which allows us to restrict our attention to bounding the
_even_ moments. This can be done via the layer cake trick
\begin{align}
  \E [|X - Y|^n]
  &=
  \int\_0^\infty
    \P (|X - Y|^n \geq u) \, \sd u
  \newline
  &\overset{\text{(i)}}{\leq}
  n \int\_0^\infty v^{n - 1} \P (|X - Y| \geq v) \, \sd v
  \newline
  &\overset{\text{(ii)}}{\leq}
  2n \int\_0^\infty v^{n - 1} \P (|X - m\_X| \geq \tfrac{v}{2}) \, \sd v
  \, ,
\end{align}
where (i) follows by the change of variable $$ u = v^n $$, and (ii) uses the
same identity as in the first solution above.

Substituting the assumed
$$ P(|X - m_X| \geq \tfrac{v}{2}) \leq c_3 e^{-c_4 t^2} $$
\begin{align}
  \E [|X - Y|^n]
  &\leq
  2n c\_3 \int\_0^\infty v^{n - 1} e^{- c\_4 \frac{v^2}{4}}
  \frac{
    \sqrt{2\pi \cdot 2 / c\_4}
  }{
    \sqrt{2\pi \cdot 2 / c\_4}
  }
  \, \sd v
  \newline
  &=
  2 n c\_3 \sqrt{\frac{\pi}{c\_4}}
  \int\_{-\infty}^{\infty}
    |v|^{n - 1}
    \frac{
      e^{- \frac{c\_4}{4} v^2}
    }{
      \sqrt{2\pi \cdot 2 / c\_4}
    }
    \, \sd v
  \newline
  &\overset{\text{(i)}}{=}
  c\_3 n
  \bigl( \tfrac{2}{\sqrt{c\_4}} \bigr)^{n}
  \Gamma \bigl( \tfrac{n}{2} \bigr)
  \, ,
\end{align}
where (i) uses the
[closed-form expression](https://en.wikipedia.org/wiki/Normal_distribution#Moments)
for absolute moments of a centred Gaussian variable
$$ Z \sim \gauss (0, \tfrac{2}{c_4}) $$
\begin{align}
  \E [|Z|^{n - 1}]
  =
  \biggl( \frac{2}{c\_4} \biggr)^{\frac{n - 1}{2}}
  \frac{
    2^{\frac{n - 1}{2}} \Gamma\bigl(\tfrac{n}{2}\bigr)
  }{\sqrt{\pi}}
\end{align}
with $$ \Gamma $$ denoting the
[Gamma function](https://en.wikipedia.org/wiki/Gamma_function).
Substituting the bound for the even moments into our MGF bound
\begin{align}
  \E [e^{\lambda(X - Y)}]
  &=
  1
  +
  \sum\_{k = 1}^{\infty}
    \frac{\lambda^{2k}}{(2k)!}
    \E [ | X - Y |^{2k} ]
  \newline
  &\overset{\text{(i)}}{\leq}
  1
  +
  c\_3
  \sum\_{k = 1}^{\infty}
    \biggl( \frac{2\lambda}{\sqrt{c\_4}} \biggr)^{2k}
    \frac{\Gamma(k)}{\Gamma (2k)}
  \newline
  &\overset{\text{(ii)}}{\leq}
  1
  +
  c\_3
  \sum\_{k = 1}^\infty
    \biggl( \frac{4 \lambda^2}{c\_4} \biggr)^k
    \frac{1}{k!}
  \newline
  &\overset{\text{(iii)}}{\leq}
  c\_3 e^{\frac{4 \lambda^2}{c\_4}}
  \, ,
\end{align}
where (i) uses the identity $$ \Gamma (2k) = (2k - 1)! $$, and (ii) follows by
the same identity and $$ \frac{1}{(2k - 1) \cdots k} \leq \frac{1}{k!} $$, and
(iii) exploits $$ c_3 \geq 1 $$ which follows by the same argument as before.

Since the above arguments can be also applied to bound
$$ \E [e^{-\lambda (X - \E [X])}]$$, we can substitute into our Chernoff bound
\begin{align}
  \P (|X - \E [X]| \geq t)
  &\leq
  2 c\_3 \inf_{\lambda > 0}
    e^{\frac{4 \lambda^2}{c\_4} - \lambda t}
  =
  c\_1 e^{- c\_2 t^2}
  \, ,
\end{align}
with $$ c_1 = 2 c_3 $$ and $$ c_2 = \frac{c_4}{8} $$.


<span class="accent">
   This bound is valid but still **does not match the required constant** in
   the exponent!
</span>


## Notes

- There is nothing special about the median in (d). In fact, we can obtain
a concentration bound for the mean similar to (d), whenever there exists
$$ a \in \R $$ such that $$ \P ( |X - a| \geq t ) \leq c_3 e^{-c_4 t^2} $$.
